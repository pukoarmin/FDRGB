{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bbce45a-ca4a-4c27-aa9a-204452f2c6d6",
   "metadata": {},
   "source": [
    "# Forest Identification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80936231-219d-40d4-bbb9-326967729162",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00a362b8-15cc-4e06-9de9-2e6fb93a50a7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-28 14:19:55.931859: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-28 14:19:56.044844: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-28 14:19:56.581785: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2024-05-28 14:19:56.581849: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2024-05-28 14:19:56.581853: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec79b40-fc90-4bcb-906e-b3fdeeb050af",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40da463b-b73f-4fd6-8110-4e4e4448e83b",
   "metadata": {},
   "source": [
    "### Function to extract a zip file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac184558-41ad-4f35-97e1-92368cd70008",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_zip(zip_path, extract_to):\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "    print(f\"Extracted {zip_path} to {extract_to}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748e1d82-cdb9-45d4-81b7-fc6bd4571d5b",
   "metadata": {},
   "source": [
    "### Function to load and preprocess images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a857127e-d4c5-41c7-b847-e191f89e8a33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_and_preprocess_images(image_folder, metadata, has_mask=False):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for _, row in metadata.iterrows():\n",
    "        img_path = os.path.join(image_folder, row['sat_image_path'])\n",
    "        if os.path.exists(img_path):\n",
    "            img = Image.open(img_path)\n",
    "            img = img.resize((128, 128))\n",
    "            img = np.array(img) / 255.0  # Normalize pixel values\n",
    "            images.append(img)\n",
    "            if has_mask:\n",
    "                mask_path = os.path.join(image_folder, row['mask_path'])\n",
    "                if os.path.exists(mask_path):\n",
    "                    label = mask_to_label(mask_path)\n",
    "                    labels.append(label)\n",
    "    images = np.array(images)\n",
    "    if has_mask:\n",
    "        labels = np.array(labels)\n",
    "        return images, labels\n",
    "    return images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340e28bf-709f-41cc-9d97-4b9728f8c4dc",
   "metadata": {},
   "source": [
    "### Function to convert mask image to label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d392ee8c-7387-47e0-bd6b-ab034621c6f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mask_to_label(mask_path):\n",
    "    mask = Image.open(mask_path).convert('RGB')\n",
    "    mask = mask.resize((128, 128), Image.NEAREST)\n",
    "    mask = np.array(mask)\n",
    "    label = np.zeros((128, 128), dtype=np.int32)\n",
    "    for rgb, idx in class_mapping.items():\n",
    "        label[np.all(mask == rgb, axis=-1)] = idx\n",
    "    return label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed98fcc8-6205-4da8-9a25-4e43af7c0714",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Define paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00491168-8a1f-460f-9030-681cadf7d02e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_zip_path = 'dataset.zip'\n",
    "dataset_path = './dataset'\n",
    "# Define paths to image folders\n",
    "train_folder = 'train'\n",
    "val_folder = 'valid'\n",
    "test_folder = 'test'\n",
    "train_mask_folder = 'train'  # Only training has masks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0614c04f-1fa6-4271-86cb-15c0e40dca33",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### Step 1: Extract the dataset (do this only once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b5ad777-4573-44c9-b14d-fb41b4e3f6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract_zip(dataset_zip_path, dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d3f4c9-b82b-4a13-ada6-85ff01e933b9",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### Step 2: Load the class dictionary and metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1d470ea-6404-4542-ad4c-f8f560c3bef0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the class dictionary\n",
    "class_dict = pd.read_csv(dataset_path + '/class_dict.csv')\n",
    "\n",
    "# Create a dictionary to map RGB values to class indices\n",
    "class_mapping = {}\n",
    "for i, row in class_dict.iterrows():\n",
    "    rgb = (row['r'], row['g'], row['b'])\n",
    "    class_mapping[rgb] = i\n",
    "\n",
    "# Load metadata\n",
    "metadata = pd.read_csv(dataset_path + '/metadata.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555d36de-d9cb-4c73-9796-bcb31f5564d6",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### Step 3: Load and preprocess the images for each split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dac92e89-a804-4513-9793-592cde408592",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = load_and_preprocess_images(dataset_path, metadata[metadata['split'] == 'train'], True)\n",
    "X_val = load_and_preprocess_images(val_folder, metadata[metadata['split'] == 'val'])\n",
    "X_test = load_and_preprocess_images(test_folder, metadata[metadata['split'] == 'test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "acdb4646-97f7-49fa-9b98-934f41d30c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preparation completed successfully.\n"
     ]
    }
   ],
   "source": [
    "np.save('X_train.npy', X_train)\n",
    "np.save('X_val.npy', X_val)\n",
    "np.save('X_test.npy', X_test)\n",
    "if train_mask_folder:\n",
    "    np.save('y_train.npy', y_train)\n",
    "\n",
    "print(\"Data preparation completed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f466670-794f-4c41-9dd4-1ceebdb64292",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d415303-1fbc-436d-8e34-b51064aa70f7",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6100e1c4-43ea-4452-b25f-653376b72b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-28 14:29:30.023545: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 12591 MB memory:  -> device: 0, name: NVIDIA A16, pci bus id: 0000:1b:00.0, compute capability: 8.6\n",
      "2024-05-28 14:29:30.027907: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13134 MB memory:  -> device: 1, name: NVIDIA A16, pci bus id: 0000:1c:00.0, compute capability: 8.6\n",
      "2024-05-28 14:29:30.029425: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 13134 MB memory:  -> device: 2, name: NVIDIA A16, pci bus id: 0000:1d:00.0, compute capability: 8.6\n",
      "2024-05-28 14:29:30.030939: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 13134 MB memory:  -> device: 3, name: NVIDIA A16, pci bus id: 0000:1e:00.0, compute capability: 8.6\n",
      "2024-05-28 14:29:30.032447: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:4 with 13134 MB memory:  -> device: 4, name: NVIDIA A16, pci bus id: 0000:ce:00.0, compute capability: 8.6\n",
      "2024-05-28 14:29:30.033976: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:5 with 13134 MB memory:  -> device: 5, name: NVIDIA A16, pci bus id: 0000:cf:00.0, compute capability: 8.6\n",
      "2024-05-28 14:29:30.035496: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:6 with 11263 MB memory:  -> device: 6, name: NVIDIA A16, pci bus id: 0000:d0:00.0, compute capability: 8.6\n",
      "2024-05-28 14:29:30.037040: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:7 with 13134 MB memory:  -> device: 7, name: NVIDIA A16, pci bus id: 0000:d1:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 128, 128, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 128, 128, 64  1792        ['input_1[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 128, 128, 64  36928       ['conv2d[0][0]']                 \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2D)   (None, 64, 64, 64)   0           ['conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 64, 64, 128)  73856       ['max_pooling2d[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 64, 64, 128)  147584      ['conv2d_2[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPooling2D)  (None, 32, 32, 128)  0          ['conv2d_3[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 32, 32, 256)  295168      ['max_pooling2d_1[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 32, 32, 256)  590080      ['conv2d_4[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling2d_2 (MaxPooling2D)  (None, 16, 16, 256)  0          ['conv2d_5[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 16, 16, 512)  1180160     ['max_pooling2d_2[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 16, 16, 512)  2359808     ['conv2d_6[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling2d_3 (MaxPooling2D)  (None, 8, 8, 512)   0           ['conv2d_7[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 8, 8, 1024)   4719616     ['max_pooling2d_3[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)              (None, 8, 8, 1024)   9438208     ['conv2d_8[0][0]']               \n",
      "                                                                                                  \n",
      " up_sampling2d (UpSampling2D)   (None, 16, 16, 1024  0           ['conv2d_9[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 16, 16, 1536  0           ['up_sampling2d[0][0]',          \n",
      "                                )                                 'conv2d_7[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)             (None, 16, 16, 512)  7078400     ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)             (None, 16, 16, 512)  2359808     ['conv2d_10[0][0]']              \n",
      "                                                                                                  \n",
      " up_sampling2d_1 (UpSampling2D)  (None, 32, 32, 512)  0          ['conv2d_11[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 32, 32, 768)  0           ['up_sampling2d_1[0][0]',        \n",
      "                                                                  'conv2d_5[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)             (None, 32, 32, 256)  1769728     ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)             (None, 32, 32, 256)  590080      ['conv2d_12[0][0]']              \n",
      "                                                                                                  \n",
      " up_sampling2d_2 (UpSampling2D)  (None, 64, 64, 256)  0          ['conv2d_13[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 64, 64, 384)  0           ['up_sampling2d_2[0][0]',        \n",
      "                                                                  'conv2d_3[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)             (None, 64, 64, 128)  442496      ['concatenate_2[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_15 (Conv2D)             (None, 64, 64, 128)  147584      ['conv2d_14[0][0]']              \n",
      "                                                                                                  \n",
      " up_sampling2d_3 (UpSampling2D)  (None, 128, 128, 12  0          ['conv2d_15[0][0]']              \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate)    (None, 128, 128, 19  0           ['up_sampling2d_3[0][0]',        \n",
      "                                2)                                'conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_16 (Conv2D)             (None, 128, 128, 64  110656      ['concatenate_3[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_17 (Conv2D)             (None, 128, 128, 64  36928       ['conv2d_16[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_18 (Conv2D)             (None, 128, 128, 7)  455         ['conv2d_17[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 31,379,335\n",
      "Trainable params: 31,379,335\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Enable eager execution for better error messages\n",
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "def unet_model(input_size=(128, 128, 3), num_classes=7):\n",
    "    inputs = Input(input_size)\n",
    "    c1 = Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    c1 = Conv2D(64, (3, 3), activation='relu', padding='same')(c1)\n",
    "    p1 = MaxPooling2D((2, 2))(c1)\n",
    "\n",
    "    c2 = Conv2D(128, (3, 3), activation='relu', padding='same')(p1)\n",
    "    c2 = Conv2D(128, (3, 3), activation='relu', padding='same')(c2)\n",
    "    p2 = MaxPooling2D((2, 2))(c2)\n",
    "\n",
    "    c3 = Conv2D(256, (3, 3), activation='relu', padding='same')(p2)\n",
    "    c3 = Conv2D(256, (3, 3), activation='relu', padding='same')(c3)\n",
    "    p3 = MaxPooling2D((2, 2))(c3)\n",
    "\n",
    "    c4 = Conv2D(512, (3, 3), activation='relu', padding='same')(p3)\n",
    "    c4 = Conv2D(512, (3, 3), activation='relu', padding='same')(c4)\n",
    "    p4 = MaxPooling2D((2, 2))(c4)\n",
    "\n",
    "    c5 = Conv2D(1024, (3, 3), activation='relu', padding='same')(p4)\n",
    "    c5 = Conv2D(1024, (3, 3), activation='relu', padding='same')(c5)\n",
    "\n",
    "    u6 = UpSampling2D((2, 2))(c5)\n",
    "    u6 = concatenate([u6, c4])\n",
    "    c6 = Conv2D(512, (3, 3), activation='relu', padding='same')(u6)\n",
    "    c6 = Conv2D(512, (3, 3), activation='relu', padding='same')(c6)\n",
    "\n",
    "    u7 = UpSampling2D((2, 2))(c6)\n",
    "    u7 = concatenate([u7, c3])\n",
    "    c7 = Conv2D(256, (3, 3), activation='relu', padding='same')(u7)\n",
    "    c7 = Conv2D(256, (3, 3), activation='relu', padding='same')(c7)\n",
    "\n",
    "    u8 = UpSampling2D((2, 2))(c7)\n",
    "    u8 = concatenate([u8, c2])\n",
    "    c8 = Conv2D(128, (3, 3), activation='relu', padding='same')(u8)\n",
    "    c8 = Conv2D(128, (3, 3), activation='relu', padding='same')(c8)\n",
    "\n",
    "    u9 = UpSampling2D((2, 2))(c8)\n",
    "    u9 = concatenate([u9, c1])\n",
    "    c9 = Conv2D(64, (3, 3), activation='relu', padding='same')(u9)\n",
    "    c9 = Conv2D(64, (3, 3), activation='relu', padding='same')(c9)\n",
    "\n",
    "    outputs = Conv2D(num_classes, (1, 1), activation='softmax')(c9)\n",
    "\n",
    "    model = Model(inputs=[inputs], outputs=[outputs])\n",
    "    return model\n",
    "\n",
    "\n",
    "# Instantiate and compile the model\n",
    "model = unet_model()\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Summary of the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f3cd1b-140b-412e-abe8-ff4f781ba1ed",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "acd44b1d-fe45-4e51-8649-e83f68330b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/structured_function.py:256: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-28 14:32:46.195003: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8400\n",
      "2024-05-28 14:32:55.149139: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 47s 256ms/step - loss: 1.4256 - accuracy: 0.5571\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 33s 256ms/step - loss: 1.3366 - accuracy: 0.5642\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 25s 255ms/step - loss: 1.2873 - accuracy: 0.5598\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 25s 254ms/step - loss: 1.2354 - accuracy: 0.5928\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 25s 254ms/step - loss: 1.2181 - accuracy: 0.5894\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 25s 251ms/step - loss: 1.1842 - accuracy: 0.5964\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 25s 250ms/step - loss: 1.1555 - accuracy: 0.5979\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 25s 251ms/step - loss: 1.1465 - accuracy: 0.5956\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 25s 252ms/step - loss: 1.0772 - accuracy: 0.6124\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 25s 254ms/step - loss: 1.0523 - accuracy: 0.6135\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 25s 253ms/step - loss: 1.0337 - accuracy: 0.6297\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 25s 249ms/step - loss: 1.0391 - accuracy: 0.6341\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 25s 250ms/step - loss: 1.0088 - accuracy: 0.6407\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 25s 250ms/step - loss: 0.9963 - accuracy: 0.6491\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 25s 251ms/step - loss: 0.9683 - accuracy: 0.6574\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 25s 252ms/step - loss: 0.9698 - accuracy: 0.6578\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 25s 252ms/step - loss: 0.9399 - accuracy: 0.6664\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 25s 251ms/step - loss: 0.9666 - accuracy: 0.6584\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 25s 249ms/step - loss: 0.9453 - accuracy: 0.6697\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 25s 249ms/step - loss: 0.9157 - accuracy: 0.6821\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 25s 250ms/step - loss: 0.9104 - accuracy: 0.6802\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 25s 250ms/step - loss: 0.8861 - accuracy: 0.6858\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 25s 249ms/step - loss: 0.9165 - accuracy: 0.6774\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 25s 250ms/step - loss: 0.8708 - accuracy: 0.6970\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 25s 249ms/step - loss: 0.8648 - accuracy: 0.6929\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 25s 250ms/step - loss: 0.8597 - accuracy: 0.7029\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 25s 250ms/step - loss: 0.8597 - accuracy: 0.7008\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 25s 250ms/step - loss: 0.8557 - accuracy: 0.6984\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 25s 250ms/step - loss: 0.8443 - accuracy: 0.6998\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 25s 252ms/step - loss: 0.8296 - accuracy: 0.7092\n"
     ]
    }
   ],
   "source": [
    "# Load the preprocessed data\n",
    "X_train = np.load('X_train.npy')\n",
    "y_train = np.load('y_train.npy')\n",
    "\n",
    "# Ensure y_train has the correct shape for sparse_categorical_crossentropy\n",
    "if len(y_train.shape) == 3:\n",
    "    y_train = np.expand_dims(y_train, axis=-1)  # Convert from (batch, height, width) to (batch, height, width, 1)\n",
    "\n",
    "# Define data augmentation\n",
    "data_gen_args = dict(rotation_range=90,\n",
    "                     width_shift_range=0.1,\n",
    "                     height_shift_range=0.1,\n",
    "                     shear_range=0.1,\n",
    "                     zoom_range=0.2,\n",
    "                     horizontal_flip=True,\n",
    "                     fill_mode='nearest')\n",
    "\n",
    "image_datagen = ImageDataGenerator(**data_gen_args)\n",
    "mask_datagen = ImageDataGenerator(**data_gen_args)\n",
    "\n",
    "# Fit the generators to the data\n",
    "image_datagen.fit(X_train, augment=True)\n",
    "mask_datagen.fit(y_train, augment=True)\n",
    "\n",
    "# Define generator to yield batches of augmented data\n",
    "def train_generator(image_datagen, mask_datagen, batch_size):\n",
    "    image_generator = image_datagen.flow(X_train, batch_size=batch_size, seed=1)\n",
    "    mask_generator = mask_datagen.flow(y_train, batch_size=batch_size, seed=1)\n",
    "    while True:\n",
    "        x_batch = image_generator.next()\n",
    "        y_batch = mask_generator.next()\n",
    "        yield x_batch, y_batch\n",
    "\n",
    "batch_size = 8\n",
    "train_gen = train_generator(image_datagen, mask_datagen, batch_size)\n",
    "    \n",
    "# Train the model with data augmentation\n",
    "history = model.fit(train_gen, steps_per_epoch=len(X_train) // batch_size, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4585c84-8292-4c7c-bd80-ed45b04d2d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training accuracy and loss values\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d582e5-06d8-4724-925e-3baaa54faa55",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f127715-7b10-4b25-b7eb-26ccb0bb1b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the preprocessed validation data\n",
    "X_val = np.load('X_val.npy')\n",
    "\n",
    "# Predict on validation set\n",
    "predictions = model.predict(X_val)\n",
    "\n",
    "# Function to visualize predictions\n",
    "def visualize_predictions(images, predictions, num_images=5):\n",
    "    for i in range(num_images):\n",
    "        plt.figure(figsize=(15, 5))\n",
    "\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.imshow(images[i])\n",
    "        plt.title('Original Image')\n",
    "\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.imshow(np.argmax(predictions[i], axis=-1), cmap='jet')\n",
    "        plt.title('Predicted Segmentation')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "# Visualize predictions\n",
    "visualize_predictions(X_val, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bed9d56-b1f8-4fde-886d-598947f8ba7d",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194d6c17-dcec-4f4c-a55b-c88465958bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_land_cover(image_path, model):\n",
    "    img = Image.open(image_path)\n",
    "    img = img.resize((128, 128))  # Resize to match the input shape of the model\n",
    "    img = np.array(img) / 255.0  # Normalize pixel values\n",
    "    img = np.expand_dims(img, axis=0)  # Add batch dimension\n",
    "    prediction = model.predict(img)\n",
    "    predicted_class = np.argmax(prediction, axis=-1)\n",
    "    return predicted_class[0]\n",
    "\n",
    "# Predict on a new image\n",
    "new_image_path = 'path_to_new_image.jpg'\n",
    "result = predict_land_cover(new_image_path, model)\n",
    "print(f\"Predicted class map:\\n{result}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
